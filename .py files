#!/usr/bin/env python3
"""
collab_gemini_demo.py

Colab / Gemini-friendly demo of the RFM (Relative Feasibility Score) meta-layer
applied to a small Branch-and-Bound knapsack solver.

Design goals:
- Short, clear, linear code that's easy for notebook assistants to read.
- Minimal dependencies (stdlib only).
- Deterministic by default (seeded RNG).
- Fast "fast" mode for quick interactive runs in Colab.
- Clear CSV + JSON outputs in /content for inspection or download.
- Safe defaults (warm-up before trusting percentile, min per-node hat).

Usage examples (in Colab):
  # write the file (see companion bash cell) and run
  python3 /content/collab_gemini_demo.py --mode cap-predictive --time 15 --items 300 --out /content/rfm_run.csv

  # quick test
  python3 /content/collab_gemini_demo.py --fast --time 6 --items 150 --instances 1

Author: assistant-generated small demo suitable for Colab/Gemini
"""
from __future__ import annotations
import argparse
import csv
import heapq
import json
import math
import random
import time
from collections import deque, namedtuple
from typing import List, Tuple, Optional

Node = namedtuple("Node", ["i", "value", "weight", "mask", "depth"])

# ---- Defaults ----
DEFAULT_CAL_P90 = 0.00018
DEFAULT_EMA_ALPHA = 0.2
DEFAULT_W_EMA = 0.6
DEFAULT_P90_WINDOW = 200
DEFAULT_PCT = 90.0
DEFAULT_S_MAX = 200
DEFAULT_THETA_P = 0.20
DEFAULT_THETA_C = 0.60
DEFAULT_TIMEOUT_MULT = 3.0
DEFAULT_BETA = 1.2
DEFAULT_M = 3
DEFAULT_VERIFY_K = 5
MIN_PER_NODE_HAT = 1e-6  # avoid zero predictions

# ---- Utilities ----
def clamp(x: float, lo: float, hi: float) -> float:
    return max(lo, min(hi, x))

def fractional_bound(start: int, value: int, weight: int, items: List[Tuple[int,int]], C: int) -> float:
    """Greedy continuous relaxation bound from index 'start'."""
    rem = C - weight
    b = float(value)
    for j in range(start, len(items)):
        w, v = items[j]
        if w <= rem:
            rem -= w
            b += v
        else:
            if w > 0:
                b += v * (rem / w)
            break
    return b

def percentile(sorted_list: List[float], p: float) -> float:
    if not sorted_list:
        return 0.0
    k = (len(sorted_list) - 1) * (p / 100.0)
    f = int(math.floor(k)); c = int(math.ceil(k))
    if f == c:
        return sorted_list[int(k)]
    return sorted_list[f] * (c - k) + sorted_list[c] * (k - f)

def generate_instance(n: int, seed: Optional[int] = None, cap_ratio: float = 0.5):
    rnd = random.Random(seed)
    items = [(rnd.randint(1, 100), rnd.randint(1, 200)) for _ in range(n)]
    items.sort(key=lambda x: x[1] / x[0], reverse=True)
    total_w = sum(w for w, _ in items)
    cap = max(1, int(total_w * cap_ratio))
    return items, cap

def greedy_seed(items: List[Tuple[int,int]], C: int) -> int:
    cap = C; total = 0
    for w, v in sorted(items, key=lambda x: x[1]/x[0], reverse=True):
        if w <= cap:
            cap -= w; total += v
    return total

# ---- Time estimator (EMA + percentile blend) ----
class TimeEstimator:
    def __init__(self, p90_window=DEFAULT_P90_WINDOW, ema_alpha=DEFAULT_EMA_ALPHA, w_ema=DEFAULT_W_EMA, calibrated_p90=DEFAULT_CAL_P90, pct=DEFAULT_PCT, warmup=5):
        self.window = deque(maxlen=p90_window)
        self.ema = float(calibrated_p90)
        self.ema_alpha = float(ema_alpha)
        self.w_ema = float(w_ema)
        self.pct = float(pct)
        self.warmup = int(warmup)

    def update(self, obs: float):
        if obs <= 0:
            return
        self.window.append(obs)
        a = self.ema_alpha
        self.ema = a * obs + (1.0 - a) * self.ema

    def per_node_hat(self) -> float:
        # until warmup reached use EMA only to be conservative
        if len(self.window) < self.warmup:
            return max(MIN_PER_NODE_HAT, float(self.ema))
        sorted_list = sorted(self.window)
        pct_val = percentile(sorted_list, self.pct) if sorted_list else self.ema
        blended = self.w_ema * self.ema + (1.0 - self.w_ema) * pct_val
        return max(MIN_PER_NODE_HAT, float(blended))

# ---- RFS helpers ----
def slack_of(B: float, incumbent: float) -> float:
    if B <= incumbent or B <= 0:
        return 0.0
    return max(0.0, (B - incumbent) / max(1.0, B))

def time_factor_of(per_node_hat: float, nodes_est: int, time_left: float) -> Tuple[float, float]:
    predicted_total = per_node_hat * max(1, int(nodes_est))
    denom = predicted_total + time_left
    if denom <= 0:
        return 0.0, predicted_total
    return clamp(time_left / denom, 0.0, 1.0), predicted_total

def compute_rfs(B: float, incumbent: float, per_node_hat: float, nodes_est: int, time_left: float):
    s = slack_of(B, incumbent)
    tf, pred = time_factor_of(per_node_hat, nodes_est, time_left)
    return s * tf, s, tf, pred

# ---- Simple B&B with RFS ----
def run_rfm_instance(items, C, T_total, config, mode="cap-predictive", node_sleep=0.0, verify_k=DEFAULT_VERIFY_K, out_csv=None):
    start = time.time()
    estimator = config.get("estimator")
    if estimator is None:
        estimator = TimeEstimator(p90_window=config.get("p90_window", DEFAULT_P90_WINDOW),
                                  ema_alpha=config.get("ema_alpha", DEFAULT_EMA_ALPHA),
                                  w_ema=config.get("w_ema", DEFAULT_W_EMA),
                                  calibrated_p90=config.get("calibrated_p90", DEFAULT_CAL_P90),
                                  pct=config.get("t_hat_percentile", DEFAULT_PCT),
                                  warmup=config.get("warmup_samples", 5))
    # seed incumbent with greedy
    incumbent = greedy_seed(items, C)
    time_to_first = 0.0 if incumbent > 0 else -1.0

    root_bound = fractional_bound(0, 0, 0, items, C)
    root = Node(0, 0, 0, 0, 0)
    heap = [(-root_bound, root)]

    stats = {"nodes_expanded": 0, "nodes_pruned_by_rfs": 0, "nodes_capped": 0}
    events = []

    consecutive = 0
    while heap:
        elapsed = time.time() - start
        time_left = T_total - elapsed
        if time_left <= 0:
            break

        # W_hat sampling (top K of heap)
        K = min(50, len(heap))
        W_hat = 0.0
        for j in range(K):
            _, n = heap[j]
            nodes_est = min(max(1, len(items) - n.i), config.get("S_max", DEFAULT_S_MAX))
            est = estimator.per_node_hat() * nodes_est
            W_hat = max(W_hat, est)

        if W_hat > config.get("beta", DEFAULT_BETA) * time_left:
            consecutive += 1
        else:
            consecutive = 0
        if consecutive >= config.get("M", DEFAULT_M):
            current_mode = "strict"
        else:
            current_mode = mode

        _, node = heapq.heappop(heap)

        # simulate node time if requested
        t0 = time.time()
        if node_sleep > 0:
            time.sleep(node_sleep)
        obs = time.time() - t0
        estimator.update(obs)
        per_hat = estimator.per_node_hat()
        pct_hat = percentile(sorted(estimator.window), config.get("t_hat_percentile", DEFAULT_PCT)) if estimator.window else estimator.ema

        nodes_est = min(max(1, len(items) - node.i), config.get("S_max", DEFAULT_S_MAX))
        B = fractional_bound(node.i, node.value, node.weight, items, C)

        rfs, slack, tf, pred_total = compute_rfs(B, incumbent, per_hat, nodes_est, time_left)
        gamma_t = clamp(time_left / max(1e-9, T_total), 0.0, 1.0)

        # per-node timeout safety
        if estimator.per_node_hat() > 0 and obs > config.get("timeout_multiplier", DEFAULT_TIMEOUT_MULT) * estimator.per_node_hat():
            events.append(("node_timeout", node.i, B, None))
            continue

        # decision
        action = "expand"
        if rfs < config.get("theta_p", DEFAULT_THETA_P):
            action = "prune_by_rfs"
            stats["nodes_pruned_by_rfs"] += 1
            events.append((action, node.i, B, rfs))
            continue
        elif rfs < config.get("theta_c", DEFAULT_THETA_C):
            effective = incumbent + (B - incumbent) * gamma_t
            stats["nodes_capped"] += 1
            action = "cap"
        else:
            effective = B

        if effective <= incumbent:
            action = "prune_by_bound"
            events.append((action, node.i, B, rfs))
            continue

        # expand
        stats["nodes_expanded"] += 1

        # keep top nodes (lightweight verify)
        # push children
        if node.i < len(items):
            w, v = items[node.i]
            # take branch
            if node.weight + w <= C:
                child_take = Node(node.i + 1, node.value + v, node.weight + w, node.mask | (1 << node.i), node.depth + 1)
                b_take = fractional_bound(child_take.i, child_take.value, child_take.weight, items, C)
                heapq.heappush(heap, (-b_take, child_take))
            # exclude
            child_excl = Node(node.i + 1, node.value, node.weight, node.mask, node.depth + 1)
            b_excl = fractional_bound(child_excl.i, child_excl.value, child_excl.weight, items, C)
            heapq.heappush(heap, (-b_excl, child_excl))

        events.append((action, node.i, B, rfs, slack, tf, pred_total, per_hat, pct_hat, incumbent, nodes_est))

    elapsed_total = time.time() - start
    summary = {
        "incumbent": incumbent,
        "nodes_expanded": stats["nodes_expanded"],
        "nodes_pruned_by_rfs": stats["nodes_pruned_by_rfs"],
        "nodes_capped": stats["nodes_capped"],
        "elapsed": elapsed_total
    }

    # write logs
    if out_csv:
        with open(out_csv, "w", newline="") as f:
            w = csv.writer(f)
            w.writerow(["event","node_i","bound","rfs","slack","time_factor","pred_total","per_node_hat","pct_hat","incumbent","nodes_est"])
            for e in events:
                # normalize tuple lengths
                if len(e) == 4:
                    w.writerow([e[0], e[1], e[2], e[3], "", "", "", "", "", "", ""])
                else:
                    w.writerow(list(e))
    return summary

# ---- Runner / CLI ----
def main():
    p = argparse.ArgumentParser(description="Colab-friendly RFM demo")
    p.add_argument("--mode", choices=["cap-predictive","strict"], default="cap-predictive")
    p.add_argument("--time", type=float, default=15.0)
    p.add_argument("--items", type=int, default=200)
    p.add_argument("--instances", type=int, default=1)
    p.add_argument("--seed", type=int, default=1)
    p.add_argument("--node-sleep", type=float, default=0.0)
    p.add_argument("--out", type=str, default="/content/rfm_collab.csv")
    p.add_argument("--fast", action="store_true", help="Reduce sizes for quick runs")
    args = p.parse_args()

    if args.fast:
        args.items = min(args.items, 150)
        args.time = min(args.time, 8.0)
        args.instances = min(args.instances, 1)

    config = {
        "w_ema": DEFAULT_W_EMA,
        "ema_alpha": DEFAULT_EMA_ALPHA,
        "p90_window": DEFAULT_P90_WINDOW,
        "t_hat_percentile": DEFAULT_PCT,
        "S_max": DEFAULT_S_MAX,
        "theta_p": DEFAULT_THETA_P,
        "theta_c": DEFAULT_THETA_C,
        "timeout_multiplier": DEFAULT_TIMEOUT_MULT,
        "beta": DEFAULT_BETA,
        "M": DEFAULT_M,
        "calibrated_p90": DEFAULT_CAL_P90,
        "warmup_samples": 5
    }

    random.seed(args.seed)
    all_summaries = []
    for inst in range(args.instances):
        seed_i = args.seed + inst
        items, C = generate_instance(args.items, seed=seed_i)
        out_path = args.out if args.instances == 1 else args.out.replace(".csv", f"_inst{inst}.csv")
        print(f"Instance {inst}: items={len(items)} capacity={C} mode={args.mode}")
        summary = run_rfm_instance(items, C, args.time, config, mode=args.mode, node_sleep=args.node_sleep, out_csv=out_path)
        print("  summary:", summary)
        all_summaries.append(summary)

    # write aggregate summary
    summary_path = args.out.replace(".csv", "_summary.json")
    with open(summary_path, "w") as jf:
        json.dump(all_summaries, jf, indent=2)
    print("Wrote summary:", summary_path)

if __name__ == "__main__":
    main()
